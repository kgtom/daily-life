
深度学习模型可以识别图像、处理自然语言，以及在部分具有挑战性的策略游戏中击败人类。在其技术发展的过程中，
现代硬件稳步推进的计算能力扮演了不可或缺的作用。很多目前最为流行的深度学习框架，如 TensorFlow、MXNet、Caffe 和 PyTorch，
支持在有限类型的服务器级 GPU 设备上获得加速，这种支持依赖于高度特化、供应商特定的 GPU 库。然而，专用深度学习加速器的种类越来越多，
这意味着现代编译器与框架越来越难以覆盖所有的硬件。

优化的四大基本挑战:

深度学习的优化编译器需要同时展示高级别与低级别的优化，在论文中，研究人员总结了在计算图级别与张量算子级别上的四大基本挑战：

1.高级数据流复写：不同的硬件设备可能具有截然不同的内存层次结构，因此，融合算子与优化数据布局的策略对于优化内存访问至关重要。

2.跨线程内存复用：现代 GPU 与专用加速器的内存可被多个计算核心共享，传统的无共享嵌套并行模式已不再是最优方法。为优化内核，在共享内存负载上的线程合作很有必要。

3.张量计算内部函数：最新的硬件带来了超越向量运算的新指令集，如 TPU 中的 GEMM 算子和英伟达 Volta 架构中的 Tensor Core。因此在调度过程中，我们必须将计算分解为张量算术内部函数，而非标量或向量代码。

4.延迟隐藏（Latency Hiding）：尽管在现代 CPU 与 GPU 上，同时拥有多线程和自动缓存管理的传统架构隐藏了延迟问题，但专用的加速器设计通常使用精简控制与分流，这为编译器堆栈的调度带来了复杂性。所以，调度仍需仔细，以隐藏内存访问延迟。

reference:
https://mp.weixin.qq.com/s/irvBbPKENiZX9G_6wh5c-Q
